"use strict";(self.webpackChunktvm_cn=self.webpackChunktvm_cn||[]).push([["72616"],{88283:function(e,n,a){a.r(n),a.d(n,{default:()=>h,frontMatter:()=>s,metadata:()=>t,assets:()=>l,toc:()=>c,contentTitle:()=>o});var t=JSON.parse('{"id":"reference/langref/relay_pattern","title":"Pattern Matching in Relay","description":"There are many places in TVM where we identify pure data-flow sub-graphs","source":"@site/versioned_docs/version-0.12.0/reference/langref/relay_pattern.md","sourceDirName":"reference/langref","slug":"/reference/langref/relay_pattern","permalink":"/docs/tvm-cn/docs/0.12.0/reference/langref/relay_pattern","draft":false,"unlisted":false,"editUrl":"https://github.com/hyperai/tvm-cn/edit/master/versioned_docs/version-0.12.0/reference/langref/relay_pattern.md","tags":[],"version":"0.12.0","lastUpdatedBy":"sparanoid","lastUpdatedAt":1744717810000,"frontMatter":{"title":"Pattern Matching in Relay"}}'),r=a("74132"),i=a("21494");let s={title:"Pattern Matching in Relay"},o=void 0,l={},c=[{value:"Pattern Examples",id:"pattern-examples",level:2},{value:"Matching One of Two Ops",id:"matching-one-of-two-ops",level:3},{value:"Matching an Op with Attributes",id:"matching-an-op-with-attributes",level:3},{value:"Matching an Optional Op",id:"matching-an-optional-op",level:3},{value:"Matching Types",id:"matching-types",level:3},{value:"Matching Non-Call Nodes",id:"matching-non-call-nodes",level:3},{value:"Matching Diamonds and Post-Dominator Graphs",id:"matching-diamonds-and-post-dominator-graphs",level:3},{value:"Matching Fuzzy Patterns",id:"matching-fuzzy-patterns",level:2},{value:"Pattern Language Design",id:"pattern-language-design",level:2},{value:"Expression Pattern",id:"expression-pattern",level:3},{value:"Wildcard",id:"wildcard",level:3},{value:"Type Pattern",id:"type-pattern",level:3},{value:"DType Pattern",id:"dtype-pattern",level:3},{value:"Shape Pattern",id:"shape-pattern",level:3},{value:"Attribute Pattern",id:"attribute-pattern",level:3},{value:"Variable Pattern",id:"variable-pattern",level:3},{value:"Alternate",id:"alternate",level:3},{value:"Domination",id:"domination",level:3},{value:"Function Pattern",id:"function-pattern",level:3},{value:"If Pattern",id:"if-pattern",level:3},{value:"Let Pattern",id:"let-pattern",level:3},{value:"Applications",id:"applications",level:2},{value:"Pattern Rewriting",id:"pattern-rewriting",level:3},{value:"Pattern Partitioning",id:"pattern-partitioning",level:3}];function d(e){let n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",p:"p",pre:"pre",...(0,i.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"There are many places in TVM where we identify pure data-flow sub-graphs\nof the Relay program and attempt to transform them in some way example\npasses include fusion, quantization, external code generation, and\ndevice specific optimizations such as bitpacking, and layer slicing used\nby VTA."}),"\n",(0,r.jsx)(n.p,{children:"Many of these passes today require a lots of boring boilerplate code in\norder to implement as well as requiring users to think in terms of\nvisitors and AST matching. Many of these transformations can easily be\ndescribed in terms of graph rewrites. In order to build a rewriter or\nother advanced machinery we first need a language of patterns to\ndescribe what we can match."}),"\n",(0,r.jsx)(n.p,{children:"Such a language is not just useful for building a rewriter but also\nproviding extension points for existing passes. For example the fusion\npass could be parameterized by a set of fusion patterns which describes\nthe capability of your hardware, and the quantization pass could take a\nset of patterns which describe which operators can be quantized on a\ngiven platform."}),"\n",(0,r.jsx)(n.p,{children:"In the backend world, we could use the same machinery to build a higher\nlevel API using bring your own code generation. This API takes set of\npatterns describing your hardware capabilities and an external compiler,\nproviding a relatively smooth heterogeneous experience out of the box."}),"\n",(0,r.jsx)(n.h2,{id:"pattern-examples",children:"Pattern Examples"}),"\n",(0,r.jsxs)(n.p,{children:["There are quite a few properties of operators that are worth matching.\nBelow we examine how to match tree properties, and expand on some use\ncases that are not fully explored in the prototype. This section\ndemonstrates how to write patterns. It is recommended to check\n",(0,r.jsx)(n.a,{href:"https://github.com/apache/tvm/blob/main/tests/python/relay/test_dataflow_pattern.py",children:"tests/python/relay/test_dataflow_pattern.py"}),"\nfor more use cases."]}),"\n",(0,r.jsx)(n.p,{children:"::: note\n::: title\nNote\n:::"}),"\n",(0,r.jsx)(n.p,{children:"If you cannot find the corresponding pattern node to match the Relay\nnode you want, you are welcome to raise an issue or submit a PR to add\nit.\n:::"}),"\n",(0,r.jsx)(n.h3,{id:"matching-one-of-two-ops",children:"Matching One of Two Ops"}),"\n",(0,r.jsx)(n.p,{children:"The first example is a simple case where we want to match one operator\nwith a single input OR another operator with a single input:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def test_match_op_or():\n    is_add_or_sub = is_op('add') | is_op('subtract')\n    assert is_add_or_sub.match(relay.op.op.get(\"add\"))\n    assert is_add_or_sub.match(relay.op.op.get(\"subtract\"))\n"})}),"\n",(0,r.jsx)(n.h3,{id:"matching-an-op-with-attributes",children:"Matching an Op with Attributes"}),"\n",(0,r.jsx)(n.p,{children:"The next example is a dense operation with any operator that is marked\nelement-wise:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def test_no_match_attr():\n    op = is_op('nn.dense').has_attr({\"TOpPattern\": K_ELEMWISE})\n    op_pat = op(wildcard(), wildcard())\n    x = relay.var('x')\n    y = relay.var('y')\n    assert not op_pat.match(relay.op.nn.dense(x, y))\n"})}),"\n",(0,r.jsx)(n.p,{children:"Here is another example to match an op with a specific attribute:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def test_match_data_layout():\n    is_conv2d = is_op('nn.conv2d')(wildcard(), wildcard()).has_attr({\"data_layout\": \"NHWC\"})\n    x = relay.var('x')\n    y = relay.var('y')\n    assert not is_conv2d.match(relay.op.nn.conv2d(x, y))\n"})}),"\n",(0,r.jsx)(n.p,{children:"Or a convolution with a specific kernel size:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def test_match_kernel_size():\n    is_conv2d = is_op(\"nn.conv2d\")(wildcard(), wildcard()).has_attr({\"kernel_size\": [3, 3]})\n    x = relay.var('x')\n    y = relay.var('y')\n    assert is_conv2d.match(relay.op.nn.conv2d(x, y, kernel_size=[3, 3]))\n"})}),"\n",(0,r.jsx)(n.h3,{id:"matching-an-optional-op",children:"Matching an Optional Op"}),"\n",(0,r.jsx)(n.p,{children:"The next example is matching a pattern with one optional operator. In\nthis pattern, we can match the graph of conv2d+bias_add+relu or the\ngraph of conv2d+bias_add."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def test_match_optional():\n    conv_node = is_op('nn.conv2d')(wildcard(), wildcard())\n    bias_node = is_op('nn.bias_add')(conv_node, wildcard())\n    pat = bias_node.optional(lambda x: is_op('nn.relu')(x))\n\n    x = relay.var('x')\n    y = relay.var('y')\n    z = relay.var('z')\n    conv2d = relay.op.nn.conv2d(x, y)\n    bias = relay.op.nn.bias_add(conv2d, z)\n    assert pat.match(bias)\n    relu = relay.op.nn.relu(bias)\n    assert pat.match(relu)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"matching-types",children:"Matching Types"}),"\n",(0,r.jsx)(n.p,{children:"In addition to matching ops with attributes, we can also make a pattern\nto match their types, in interms of the shape and data type. Here are\nsome examples:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def test_match_type():\n    # Match any op with float32\n    pat1 = has_dtype('float32')\n    x = relay.var('x', shape=(10, 10), dtype='float32')\n    assert pat1.match(x)\n\n    # Match any op with shape (10, 10)\n    pat2 = has_shape((10, 10))\n    x = relay.var('x', shape=(10, 10), dtype='float32')\n    assert pat2.match(x)\n\n    # Match conv2d+relu with a certain shape\n    conv2d = is_op('nn.conv2d')(wildcard(), wildcard())\n    pat3 = is_op('nn.relu')(conv2d).has_shape((1, 32, 28, 28))\n\n    x = relay.var('x', shape=(1, 3, 28, 28), dtype='float32')\n    w = relay.var('w', shape=(32, 3, 3, 3), dtype='float32')\n    conv2d = relay.nn.conv2d(x, w, strides=(1, 1), padding=(1, 1))\n    relu = relay.nn.relu(conv2d)\n    assert pat3.match(relu)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"matching-non-call-nodes",children:"Matching Non-Call Nodes"}),"\n",(0,r.jsx)(n.p,{children:"Sometimes we may also want to match a pattern that includes Tuple or\nTupleGetItem nodes. Since there are not call nodes, we need to use\nspecific pattern nodes to match them:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def test_match_tuple():\n    x = relay.var('x')\n    y = relay.var('y')\n    z = relay.var('z')\n    tuple_pattern = is_tuple((wildcard(), wildcard(), wildcard()))\n    assert tuple_pattern.match(relay.expr.Tuple((x,y,z)))\n"})}),"\n",(0,r.jsx)(n.p,{children:"The next example is matching a pattern of batch_norm -> get(0) ->\nrelu. Note that you can also use\n[is_tuple_get_item(bn_node)] to match a\n[TupleGetItem] node with any index."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def test_match_tuple_get_item():\n    bn_node = is_op(\'nn.batch_norm\')(wildcard(), wildcard(), wildcard(), wildcard(), wildcard())\n    tuple_get_item_node = is_tuple_get_item(bn_node, 0)\n    pat = is_op(\'nn.relu\')(tuple_get_item_node)\n\n    x = relay.var(\'x\', shape=(1, 8))\n    gamma = relay.var("gamma", shape=(8,))\n    beta = relay.var("beta", shape=(8,))\n    moving_mean = relay.var("moving_mean", shape=(8,))\n    moving_var = relay.var("moving_var", shape=(8,))\n    bn_node = relay.nn.batch_norm(x, gamma, beta, moving_mean, moving_var)\n    tuple_get_item_node = bn_node[0]\n    out = relay.nn.relu(tuple_get_item_node)\n    pat.match(out)\n'})}),"\n",(0,r.jsx)(n.p,{children:"If we have a pattern that crosses a function boundary, we might want to\nmatch the Function itself"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def test_match_func():\n    x = relay.var("x")\n    y = relay.var("y")\n    wc1 = wildcard()\n    wc2 = wildcard()\n    func_pattern = FunctionPattern([wc1, wc2], wc1 + wc2)\n    assert func_pattern.match(relay.Function([x, y], x + y))\n'})}),"\n",(0,r.jsx)(n.p,{children:"The next example is matching a constant node regarding its values. This\nis useful to check if a specific parameter in a subgraph has been bound\nor not."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def test_match_constant():\n    conv2d = is_op('nn.conv2d')(wildcard(), is_constant())\n    pattern = is_op('nn.bias_add')(conv2d, wildcard())\n\n    x = relay.var('x', shape=(1, 3, 224, 224))\n    w = relay.var('w', shape=(3, 3, 3, 3))\n    b = relay.var('b', shape=(3, ))\n    conv2d = relay.op.nn.conv2d(x, w)\n    out = relay.op.nn.bias_add(conv2d, b)\n    func = relay.Function([x, w, b], out)\n    mod = tvm.IRModule.from_expr(func)\n\n    # Two inputs of the conv2d in the graph are VarNode by default, so no match.\n    assert not pattern.match(mod['main'].body)\n\n    # The second input (weight) has been bind with constant values so it is now a constant node.\n    mod[\"main\"] = bind_params_by_name(mod[\"main\"],\n                                    {'w': tvm.nd.array(np.ones(shape=(3, 3, 3, 3)))})\n    assert pattern.match(mod['main'].body)\n"})}),"\n",(0,r.jsxs)(n.p,{children:["On the other hand, if you need to match the constant with a specific\nvalue, you can directly use ",(0,r.jsx)(n.code,{children:"is_expr"}),". This could be useful for\nalgebraic simplify."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def test_match_plus_zero():\n    zero = (is_expr(relay.const(0)) | is_expr(relay.const(0.0)))\n    pattern = wildcard() + zero\n\n    x = relay.Var('x')\n    y = x + relay.const(0)\n    assert pattern.match(y)\n"})}),"\n",(0,r.jsx)(n.p,{children:"The next example is matching function nodes with a specific attribute:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def test_match_function():\n    pattern = wildcard().has_attr({"Composite": "add"})\n\n    x = relay.var(\'x\')\n    y = relay.var(\'y\')\n    f = relay.Function([x, y], x + y).with_attr("Composite", "add")\n    assert pattern.match(f)\n'})}),"\n",(0,r.jsxs)(n.p,{children:["A Relay ",(0,r.jsx)(n.code,{children:"If"})," expression can be matched if all of its condition, true\nbranch and false branch are matched:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def test_match_if():\n    x = is_var("x")\n    y = is_var("y")\n    pat = is_if(is_op("less")(x, y), x, y)\n\n    x = relay.var("x")\n    y = relay.var("y")\n    cond = x < y\n\n    assert pat.match(relay.expr.If(cond, x, y))\n'})}),"\n",(0,r.jsxs)(n.p,{children:["A Relay ",(0,r.jsx)(n.code,{children:"Let"})," expression can be matched if all of its variable, value,\nand body are matched:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def test_match_let():\n    x = is_var("x")\n    y = is_var("y")\n    let_var = is_var("let")\n    pat = is_let(let_var, is_op("less")(x, y), let_var)\n\n    x = relay.var("x")\n    y = relay.var("y")\n    lv = relay.var("let")\n    cond = x < y\n    assert pat.match(relay.expr.Let(lv, cond, lv))\n'})}),"\n",(0,r.jsx)(n.h3,{id:"matching-diamonds-and-post-dominator-graphs",children:"Matching Diamonds and Post-Dominator Graphs"}),"\n",(0,r.jsx)(n.p,{children:"The next example is matching a diamond with two inputs at the top of the\ndiamond:"}),"\n",(0,r.jsx)(n.p,{children:"def test_match_diamond():"}),"\n",(0,r.jsx)(n.h1,{id:"pattern",children:"Pattern"}),"\n",(0,r.jsx)(n.p,{children:"is_conv2d = is_op('nn.conv2d')(is_var(), is_var())\npath1 = is_op('nn.relu')(is_conv2d)\npath2 = is_op('nn.leaky_relu')(is_conv2d)\ndiamond = is_op('add')(path1, path2)"}),"\n",(0,r.jsx)(n.h1,{id:"expr",children:"Expr"}),"\n",(0,r.jsx)(n.p,{children:"inp = relay.var('input')\nweight = relay.var('weight')\nconv2d = relay.op.nn.conv2d(inp, weight)\nrelu = relay.op.nn.relu(conv2d)\nleaky_relu = relay.op.nn.leaky_relu(conv2d, alpha=0)\nout = relu + leaky_relu"}),"\n",(0,r.jsx)(n.h1,{id:"check",children:"Check"}),"\n",(0,r.jsx)(n.p,{children:"assert diamond.match(out)"}),"\n",(0,r.jsx)(n.p,{children:"The final example is matching diamonds with a post-dominator\nrelationship. We embed dominator analysis as type of matching in the\npattern language in order to allow for pattern matching with unknown\ntopology. This is important because we want to be able to use the\nlanguage to describe fuse patterns, like elementwise operations followed\nby a conv2d:"}),"\n",(0,r.jsx)(n.p,{children:"def test_match_dom_diamond():"}),"\n",(0,r.jsx)(n.h1,{id:"pattern-1",children:"Pattern"}),"\n",(0,r.jsx)(n.p,{children:"is_conv2d = is_op('nn.conv2d')(is_var(), is_var())\nreduction = is_op('add')(wildcard(), wildcard())\ndiamond = dominates(is_conv2d, is_elemwise, reduction)"}),"\n",(0,r.jsx)(n.h1,{id:"expr-1",children:"Expr"}),"\n",(0,r.jsx)(n.p,{children:"inp = relay.var('input')\nweight = relay.var('weight')\nconv2d = relay.op.nn.conv2d(inp, weight)\nrelu = relay.op.nn.relu(conv2d)\nleaky_relu = relay.op.nn.leaky_relu(conv2d, alpha=0)\nout = relu + leaky_relu"}),"\n",(0,r.jsx)(n.h1,{id:"check-1",children:"Check"}),"\n",(0,r.jsx)(n.p,{children:"assert diamond.match(out)"}),"\n",(0,r.jsx)(n.h2,{id:"matching-fuzzy-patterns",children:"Matching Fuzzy Patterns"}),"\n",(0,r.jsx)(n.p,{children:'The Dominator analysis above lets one match a subgraph of Relay AST that\ndoesn\'t correspond to a set of patterns nodes exactly 1-to-1. There are\na few other places where we support such "fuzzy" matching.'}),"\n",(0,r.jsx)(n.p,{children:"Tuples, Functions, and Call nodes with any number of inputs can be\nmatched by passing [None] as the argument value, i.e.:"}),"\n",(0,r.jsx)(n.p,{children:"tuple_pattern = is_tuple(None)\nfunc_pattern = FunctionPattern(None, wildcard() + wildcard())\ncall_pattern = func_pattern(None)"}),"\n",(0,r.jsx)(n.p,{children:"These patterns allow matching more generic classes patterns by\nconstraining the use of the arguments rather than the number of\narguments."}),"\n",(0,r.jsx)(n.p,{children:"Additionally, we support matching Functions with fuzzy bodies, i.e., a\nfunction body that is under constrained by the pattern. The pattern\n[FunctionPattern([is_var(), is_var()], wildcard() +\nwildcard()])] will match [relay.Function([x, y], x +\ny)], but it will also match [relay.Function([x, y], x *\nx + y)]. In the second case, the pattern doesn't perfectly\nconstrain the body of the function, so the resulting match is fuzzy."}),"\n",(0,r.jsx)(n.h2,{id:"pattern-language-design",children:"Pattern Language Design"}),"\n",(0,r.jsx)(n.p,{children:"The pattern language proposed is designed to be a mirror of Relay's IR\nwith additional support for common scenarios. The goal of the pattern\nlanguage is to provide a regular-expression like capability for matching\ndata-flow graphs and doing rewriting."}),"\n",(0,r.jsx)(n.p,{children:"The high level design is to introduce a language of patterns for now we\npropose the language as:"}),"\n",(0,r.jsxs)(n.p,{children:["Pattern ::= expr\n| *\n| pattern(pattern1, ... patternN)\n| has_type(type)\n| has_dtype(type)\n| has_shape(shape)\n| has_attr(attrs)\n| is_var(name)\n| is_constant()\n| is_expr(expr)\n| is_op(op_name)\n| is_tuple()\n| is_tuple_get_item(pattern, index = None)\n| is_if(cond, tru, fls)\n| is_let(var, value, body)\n| pattern1 ",(0,r.jsx)(n.code,{children:"|"})," pattern2\n| dominates(parent_pattern, path_pattern, child_pattern)\n| FunctionPattern(params, body)"]}),"\n",(0,r.jsx)(n.p,{children:"The above language then provides a matching interface with both can\nselect sub-graphs as well as verify that the graph does match the\npattern."}),"\n",(0,r.jsx)(n.h3,{id:"expression-pattern",children:"Expression Pattern"}),"\n",(0,r.jsx)(n.p,{children:"Match a literal expression."}),"\n",(0,r.jsx)(n.h3,{id:"wildcard",children:"Wildcard"}),"\n",(0,r.jsx)(n.p,{children:"Match any expression."}),"\n",(0,r.jsx)(n.h3,{id:"type-pattern",children:"Type Pattern"}),"\n",(0,r.jsx)(n.p,{children:"Check that the expression matched by the nested pattern has a particular\ntype."}),"\n",(0,r.jsx)(n.h3,{id:"dtype-pattern",children:"DType Pattern"}),"\n",(0,r.jsx)(n.p,{children:"Check that the expression matched by the nested pattern has a particular\ndata type."}),"\n",(0,r.jsx)(n.h3,{id:"shape-pattern",children:"Shape Pattern"}),"\n",(0,r.jsx)(n.p,{children:"Check that the expression matched by the nested pattern has a particular\noutput shape."}),"\n",(0,r.jsx)(n.h3,{id:"attribute-pattern",children:"Attribute Pattern"}),"\n",(0,r.jsx)(n.p,{children:"Check that the operator matched by the pattern has an attribute with a\nparticular value."}),"\n",(0,r.jsx)(n.h3,{id:"variable-pattern",children:"Variable Pattern"}),"\n",(0,r.jsx)(n.p,{children:"Check that the expression is a relay Variable, and optional provide a\nname to match to the Variable name."}),"\n",(0,r.jsx)(n.h3,{id:"alternate",children:"Alternate"}),"\n",(0,r.jsx)(n.p,{children:"Either match the first pattern or the second pattern."}),"\n",(0,r.jsx)(n.h3,{id:"domination",children:"Domination"}),"\n",(0,r.jsx)(n.p,{children:"Match child pattern, find a match for the parent pattern, insuring that\nthe child ultimately dominates the parent (i.e., no nodes outside the\npattern use outputs of the parent), and that ever node between the child\nand the pattern matches the path pattern."}),"\n",(0,r.jsx)(n.h3,{id:"function-pattern",children:"Function Pattern"}),"\n",(0,r.jsx)(n.p,{children:"Match a Function with a body and parameters"}),"\n",(0,r.jsx)(n.h3,{id:"if-pattern",children:"If Pattern"}),"\n",(0,r.jsx)(n.p,{children:"Match an If with condition, true branch, and false branch"}),"\n",(0,r.jsx)(n.h3,{id:"let-pattern",children:"Let Pattern"}),"\n",(0,r.jsx)(n.p,{children:"Match a Let with a variable, value, and body"}),"\n",(0,r.jsx)(n.h2,{id:"applications",children:"Applications"}),"\n",(0,r.jsx)(n.p,{children:"The pattern language provides not only the pattern matching but also\npattern processing. Here we introduce two pattern processing approaches\nand provide some examples."}),"\n",(0,r.jsx)(n.h3,{id:"pattern-rewriting",children:"Pattern Rewriting"}),"\n",(0,r.jsxs)(n.p,{children:["If you would like to replace the matched pattern with another subgraph,\nyou can leverage the ",(0,r.jsx)(n.code,{children:"rewrite"})," transformation. Here is an example of\nrewriting a series of arithmetic operators with a single batch_norm op.\nThe constructor parameter ",(0,r.jsx)(n.code,{children:"require_type"})," indicates whether InferType is\nrequired to be run before the callback."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"class BatchnormCallback(DFPatternCallback):\n    # A callback class to rewrite the matched pattern to a batch_norm op.\n    def __init__(self, require_type=False):\n        super().__init__(require_type)\n        self.x = wildcard()\n        self.var = wildcard()\n        self.mean = wildcard()\n        self.beta = wildcard()\n        self.gamma = wildcard()\n        self.eps = wildcard()\n\n        self.pattern = self.gamma * (self.x - self.mean)/is_op(\"sqrt\")(self.var + self.eps) + self.beta\n\n    def callback(self, pre, post, node_map):\n        x = node_map[self.x][0]\n        var = node_map[self.var][0]\n        mean = node_map[self.mean][0]\n        beta = node_map[self.beta][0]\n        gamma = node_map[self.gamma][0]\n        eps = node_map[self.eps][0]\n        return relay.op.nn.batch_norm(x, gamma, beta, mean, var, epsilon = eps.data.numpy().item())[0]\n\n    # A graph of arithmetic operators that are functional equivalent to batch_norm.\n    x = relay.var('x')\n    var = relay.var('var')\n    mean = relay.var('mean')\n    beta = relay.var('beta')\n    gamma = relay.var('gamma')\n    BN = gamma * (x - mean)/relay.op.sqrt(var + relay.const(1e-5)) + beta\n\n    from tvm.relay.dataflow_pattern import rewrite\n    out = rewrite(BatchnormCallback(), BN)\n    assert tvm.ir.structural_equal(out, relay.op.nn.batch_norm(x, gamma, beta, mean, var, epsilon = 1e-5)[0])\n"})}),"\n",(0,r.jsxs)(n.p,{children:["The function ",(0,r.jsx)(n.code,{children:"def callback(self, pre, post, node_map)"})," will be invoked\nwhen the rewriter matches ",(0,r.jsx)(n.code,{children:"self.pattern"}),". ",(0,r.jsx)(n.code,{children:"node_map"})," is a dictionary\nmapping from pattern nodes to matched nodes in the graph."]}),"\n",(0,r.jsxs)(n.p,{children:["The callback function will be invoked recursively on the returned\npattern until the pattern stops changing. As a result, if ",(0,r.jsx)(n.code,{children:"self.pattern"}),"\nmatches any part of the graph that the callback returned, the rewriter\nwill run in a loop. If you want to avoid multiple rewrites, you can pass\na ",(0,r.jsx)(n.code,{children:"rewrite_once=True"})," parameter to the constructor."]}),"\n",(0,r.jsx)(n.h3,{id:"pattern-partitioning",children:"Pattern Partitioning"}),"\n",(0,r.jsxs)(n.p,{children:["If you would like to perform a more complex processing for matched\nsubgraphs and you are not satisfied with ",(0,r.jsx)(n.code,{children:"rewrite"}),", you may consider\npartitioning the matched subgraphs to a separate Relay function and\nperform other processes to the function. Here we use ",(0,r.jsx)(n.code,{children:"pattern.partition"}),"\nto create a new Relay function for each matched subgraph. The\nfunctionality is similar to the op fusion pass in TVM:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# A pattern matching conv2d+relu.\npattern = is_op(\"nn.relu\")(is_op(\"nn.conv2d\")(wildcard(), wildcard()))\n\n# A graph.\nx = relay.var('input')\nw = relay.var('weight')\nconv2d = relay.op.nn.conv2d(x, w)\nrelu = relay.op.nn.relu(conv2d)\nprint('relu')\n# free_var %x: Tensor[(1, 3, 224, 224), float32]\n# free_var %w: Tensor[(3, 3, 3, 3), float32]\n# %0 = nn.conv2d(%x, %w, padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 3, 222, 222), float32] */;\n# free_var %b: Tensor[(3), float32]\n# nn.bias_add(%0, %b) /* ty=Tensor[(1, 3, 222, 222), float32] */\n\n# After partition.\nprint(pattern.partition(relu))\n# free_var %x: Tensor[(1, 3, 224, 224), float32]\n# free_var %w: Tensor[(3, 3, 3, 3), float32]\n# free_var %b: Tensor[(3), float32]\n# %1 = fn (%FunctionVar_0_0, %FunctionVar_0_1,\n#          %FunctionVar_0_2, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_\") {\n#   %0 = nn.conv2d(%FunctionVar_0_0, %FunctionVar_0_1, padding=[0, 0, 0, 0]);\n#   nn.bias_add(%0, %FunctionVar_0_2)\n# };\n# %1(%x, %w, %b)\n"})}),"\n",(0,r.jsx)(n.p,{children:"Note that you can also specify the attributes for the created functions:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"print(pattern.partition(relu, {'Composite': 'one_layer'}))\n# free_var %x: Tensor[(1, 3, 224, 224), float32]\n# free_var %w: Tensor[(3, 3, 3, 3), float32]\n# free_var %b: Tensor[(3), float32]\n# %1 = fn (%FunctionVar_0_0, %FunctionVar_0_1,\n#          %FunctionVar_0_2, Composite=\"one_layer\",\n#                            PartitionedFromPattern=\"nn.conv2d_nn.bias_add_\") {\n#   %0 = nn.conv2d(%FunctionVar_0_0, %FunctionVar_0_1, padding=[0, 0, 0, 0]);\n#   nn.bias_add(%0, %FunctionVar_0_2)\n# };\n# %1(%x, %w, %b)\n"})}),"\n",(0,r.jsxs)(n.p,{children:["If you need a customized checking function that cannot be specified\nusing pattern language, you can specify ",(0,r.jsx)(n.code,{children:"check"})," function when\npartitioning. The following example demonstrates a case that checks\ninput data layout of a subgraph:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def check(pre):\n    conv = pre.args[0]\n    return (conv.attrs.data_layout == "NCHW") and bool(conv.checked_type.shape[0] == 1)\n\npattern.partition(relu, check=check)\n'})}),"\n",(0,r.jsxs)(n.p,{children:["In this example, we check if the first argument of the matched subgraph\n(i.e., ",(0,r.jsx)(n.code,{children:"pre.args[0]"}),') has data layout "NCHW" and if its batch size\nis 1. This feature is useful if the conditions of matching a pattern\ncannot be verified by analyzing the pattern itself.']})]})}function h(e={}){let{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},21494:function(e,n,a){a.d(n,{Z:function(){return o},a:function(){return s}});var t=a(39546);let r={},i=t.createContext(r);function s(e){let n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);