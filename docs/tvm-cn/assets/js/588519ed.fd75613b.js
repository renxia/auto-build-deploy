"use strict";(self.webpackChunktvm_cn=self.webpackChunktvm_cn||[]).push([["62817"],{96088:function(n,e,t){t.r(e),t.d(e,{default:()=>_,frontMatter:()=>i,metadata:()=>a,assets:()=>s,toc:()=>c,contentTitle:()=>d});var a=JSON.parse('{"id":"how_to/optimize/tensorcore_conv","title":"\u5982\u4F55\u4F7F\u7528 TensorCores \u4F18\u5316\u5377\u79EF","description":"\u5355\u51FB \u6B64\u5904 \u4E0B\u8F7D\u5B8C\u6574\u7684\u793A\u4F8B\u4EE3\u7801","source":"@site/versioned_docs/version-0.10.0/how_to/optimize/03-tensorcore_conv.md","sourceDirName":"how_to/optimize","slug":"/how_to/optimize/tensorcore_conv","permalink":"/docs/tvm-cn/docs/0.10.0/how_to/optimize/tensorcore_conv","draft":false,"unlisted":false,"editUrl":"https://github.com/hyperai/tvm-cn/edit/master/versioned_docs/version-0.10.0/how_to/optimize/03-tensorcore_conv.md","tags":[],"version":"0.10.0","lastUpdatedBy":"sparanoid","lastUpdatedAt":1744717810000,"sidebarPosition":3,"frontMatter":{"title":"\u5982\u4F55\u4F7F\u7528 TensorCores \u4F18\u5316\u5377\u79EF"},"sidebar":"tutorialSidebar","previous":{"title":"\u5982\u4F55\u5728 GPU \u4E0A\u4F18\u5316\u5377\u79EF","permalink":"/docs/tvm-cn/docs/0.10.0/how_to/optimize/gpu_conv"},"next":{"title":"\u4F7F\u7528\u6A21\u677F\u548C AutoTVM \u8FDB\u884C\u81EA\u52A8\u8C03\u4F18","permalink":"/docs/tvm-cn/docs/0.10.0/how_to/autotune/"}}'),r=t("74132"),o=t("21494");let i={title:"\u5982\u4F55\u4F7F\u7528 TensorCores \u4F18\u5316\u5377\u79EF"},d="\u5982\u4F55\u4F7F\u7528 TensorCores \u4F18\u5316\u5377\u79EF",s={},c=[{value:"TensorCore \u4ECB\u7ECD",id:"tensorcore-\u4ECB\u7ECD",level:2},{value:"\u51C6\u5907\u548C\u7B97\u6CD5",id:"\u51C6\u5907\u548C\u7B97\u6CD5",level:2},{value:"\u5185\u5B58\u8303\u56F4",id:"\u5185\u5B58\u8303\u56F4",level:2},{value:"\u5B9A\u4E49\u5F20\u91CF\u5185\u8054\u51FD\u6570",id:"\u5B9A\u4E49\u5F20\u91CF\u5185\u8054\u51FD\u6570",level:2},{value:"\u8C03\u5EA6\u8BA1\u7B97",id:"\u8C03\u5EA6\u8BA1\u7B97",level:2},{value:"\u5C06\u8BA1\u7B97\u964D\u7EA7\u4E3A\u5185\u8054\u51FD\u6570",id:"\u5C06\u8BA1\u7B97\u964D\u7EA7\u4E3A\u5185\u8054\u51FD\u6570",level:2},{value:"\u751F\u6210 CUDA \u5185\u6838",id:"\u751F\u6210-cuda-\u5185\u6838",level:2},{value:"\u603B\u7ED3",id:"\u603B\u7ED3",level:2}];function l(n){let e={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",strong:"strong",...(0,o.a)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"\u5982\u4F55\u4F7F\u7528-tensorcores-\u4F18\u5316\u5377\u79EF",children:"\u5982\u4F55\u4F7F\u7528 TensorCores \u4F18\u5316\u5377\u79EF"})}),"\n",(0,r.jsx)(e.admonition,{type:"note",children:(0,r.jsxs)(e.p,{children:["\u5355\u51FB ",(0,r.jsx)(e.a,{href:"https://tvm.apache.org/docs/how_to/optimize_operators/opt_conv_tensorcore.html#sphx-glr-download-how-to-optimize-operators-opt-conv-tensorcore-py",children:"\u6B64\u5904"})," \u4E0B\u8F7D\u5B8C\u6574\u7684\u793A\u4F8B\u4EE3\u7801"]})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"\u4F5C\u8005"}),"\uFF1A",(0,r.jsx)(e.a,{href:"https://github.com/Hzfengsy",children:"Siyuan Feng"})]}),"\n",(0,r.jsxs)(e.p,{children:["\u672C\u6559\u7A0B\u6F14\u793A\u5982\u4F55\u5728 TVM \u4E2D\u4F7F\u7528 TensorCores \u7F16\u5199\u9AD8\u6027\u80FD\u5377\u79EF\u8C03\u5EA6\u3002\u5728\u8FD9\u4E2A\u4F8B\u5B50\u4E2D\uFF0C\u4F1A\u5047\u8BBE\u5377\u79EF\u8F93\u5165\u7684 batch \u8F83\u5927\u3002\u5F3A\u70C8\u5EFA\u8BAE\u524D\u7F6E\u8BB2\u89E3 ",(0,r.jsx)(e.a,{href:"gpu_conv",children:"\u5982\u4F55\u5728 GPU \u4E0A\u4F18\u5316\u5377\u79EF"}),"\u3002"]}),"\n",(0,r.jsx)(e.h2,{id:"tensorcore-\u4ECB\u7ECD",children:"TensorCore \u4ECB\u7ECD"}),"\n",(0,r.jsxs)(e.p,{children:["\u6BCF\u4E2A Tensor Core \u90FD\u63D0\u4F9B\u4E86\u4E00\u4E2A 4x4x4 \u77E9\u9635\u5904\u7406\u6570\u7EC4\uFF0C\u5B83\u4F7F\u5F97 ",(0,r.jsx)(e.code,{children:"D = A * B + C"}),"\uFF0C\u5176\u4E2D A\u3001B\u3001C \u548C D \u662F 4x4 \u77E9\u9635\uFF0C\u5982\u56FE\u6240\u793A\u3002\u77E9\u9635\u4E58\u6CD5\u8F93\u5165 A \u548C B \u662F FP16 \u77E9\u9635\uFF0C\u800C\u7D2F\u52A0\u77E9\u9635 C \u548C D \u53EF\u4EE5\u662F FP16 \u6216 FP32 \u77E9\u9635\u3002"]}),"\n",(0,r.jsxs)(e.p,{children:["\u4F46\u662F\uFF0CCUDA \u5F00\u53D1\u8005\u53EA\u80FD\u4F7F\u7528 warp \u7EA7\u539F\u8BED ",(0,r.jsx)(e.code,{children:"wmma::mma_sync(acc_frag, a_frag, b_frag, acc_frag)"})," \u5728\u5F20\u91CF\u6838\u4E0A\u6267\u884C 16x16x16 \u534A\u7CBE\u5EA6\u77E9\u9635\u4E58\u6CD5\u3002\u8C03\u7528\u77E9\u9635\u4E58\u6CD5\u4E4B\u524D\uFF0C\u5F00\u53D1\u8005\u5FC5\u987B\u4F7F\u7528\u539F\u59CB ",(0,r.jsx)(e.code,{children:"wmma::load_matrix_sync"})," \u663E\u5F0F\u5730\u5C06\u6570\u636E\u4ECE\u5185\u5B58\u52A0\u8F7D\u5230\u5BC4\u5B58\u5668\u4E2D\u3002NVCC \u7F16\u8BD1\u5668\u5C06\u8BE5\u539F\u8BED\u8F6C\u6362\u4E3A\u591A\u4E2A\u5185\u5B58\u52A0\u8F7D\u6307\u4EE4\u3002\u5728\u8FD0\u884C\u65F6\uFF0C\u6BCF\u4E2A\u7EBF\u7A0B\u4ECE\u77E9\u9635 A \u52A0\u8F7D 16 \u4E2A\u5143\u7D20\uFF0C\u4ECE B \u52A0\u8F7D 16 \u4E2A\u5143\u7D20\u3002"]}),"\n",(0,r.jsx)(e.h2,{id:"\u51C6\u5907\u548C\u7B97\u6CD5",children:"\u51C6\u5907\u548C\u7B97\u6CD5"}),"\n",(0,r.jsx)(e.p,{children:"\u5BF9\u5177\u6709 256 \u4E2A\u901A\u9053\u548C 14 x 14 \u7EF4\u5EA6\u7684\u8F93\u5165\u5F20\u91CF\u4F7F\u7528\u56FA\u5B9A\u5927\u5C0F\u3002batch size \u4E3A 256\uFF0C\u5377\u79EF\u8FC7\u6EE4\u5668\u5305\u542B 512 \u4E2A\u5927\u5C0F\u4E3A 3 x 3 \u7684\u8FC7\u6EE4\u5668\uFF0C\u4F7F\u7528 stride size \u4E3A 1 \u548C padding size \u4E3A 1 \u8FDB\u884C\u5377\u79EF\u3002\u5728\u793A\u4F8B\u4E2D\uFF0C\u4F7F\u7528 NHWCnc \u5185\u5B58\u5E03\u5C40\u3002\u4EE5\u4E0B\u4EE3\u7801\u5B9A\u4E49\u4E86 TVM \u4E2D\u7684\u5377\u79EF\u7B97\u6CD5\u3002"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import tvm\nfrom tvm import te\nimport numpy as np\nfrom tvm.contrib import nvcc\n\n# \u8F93\u5165\u548C\u8FC7\u6EE4\u5668\u7684\u5927\u5C0F\nbatch_size = 256\nheight = 14\nwidth = 14\nin_channels = 256\nout_channels = 512\nkernel_h = 3\nkernel_w = 3\npad_h = 1\npad_w = 1\nstride_h = 1\nstride_w = 1\n\n# TensorCore shape\nblock_size = 16\n\nassert batch_size % block_size == 0\nassert in_channels % block_size == 0\nassert out_channels % block_size == 0\n\n# \u8F93\u5165\u7279\u5F81\u56FE\uFF1A\uFF08N\uFF0CH\uFF0CW\uFF0CIC\uFF0Cn\uFF0Cic\uFF09\ndata_shape = (\n    batch_size // block_size,\n    height,\n    width,\n    in_channels // block_size,\n    block_size,\n    block_size,\n)\n# Kernel: (H, W, IC, OC, ic, oc)\nkernel_shape = (\n    kernel_h,\n    kernel_w,\n    in_channels // block_size,\n    out_channels // block_size,\n    block_size,\n    block_size,\n)\n# \u8F93\u51FA\u7279\u5F81\u56FE\uFF1A(N, H, W, OC, n, oc)\noutput_shape = (\n    batch_size // block_size,\n    height,\n    width,\n    out_channels // block_size,\n    block_size,\n    block_size,\n)\n\n# Reduction axes\nkh = te.reduce_axis((0, kernel_h), name="kh")\nkw = te.reduce_axis((0, kernel_w), name="kw")\nic = te.reduce_axis((0, in_channels // block_size), name="ic")\nii = te.reduce_axis((0, block_size), name="ii")\n\n# \u7B97\u6CD5\nA = te.placeholder(data_shape, name="A", dtype="float16")\nW = te.placeholder(kernel_shape, name="W", dtype="float16")\nApad = te.compute(\n    (\n        batch_size // block_size,\n        height + 2 * pad_h,\n        width + 2 * pad_w,\n        in_channels // block_size,\n        block_size,\n        block_size,\n    ),\n    lambda n, h, w, i, nn, ii: tvm.tir.if_then_else(\n        tvm.tir.all(h >= pad_h, h - pad_h < height, w >= pad_w, w - pad_w < width),\n        A[n, h - pad_h, w - pad_w, i, nn, ii],\n        tvm.tir.const(0.0, "float16"),\n    ),\n    name="Apad",\n)\nConv = te.compute(\n    output_shape,\n    lambda n, h, w, o, nn, oo: te.sum(\n        Apad[n, h * stride_h + kh, w * stride_w + kw, ic, nn, ii].astype("float32")\n        * W[kh, kw, ic, o, ii, oo].astype("float32"),\n        axis=[ic, kh, kw, ii],\n    ),\n    name="Conv",\n)\n\ns = te.create_schedule(Conv.op)\ns[Apad].compute_inline()\n'})}),"\n",(0,r.jsx)(e.h2,{id:"\u5185\u5B58\u8303\u56F4",children:"\u5185\u5B58\u8303\u56F4"}),"\n",(0,r.jsxs)(e.p,{children:["\u4F20\u7EDF\u7684 GPU \u8C03\u5EA6\u6709\u5168\u5C40\u3001\u5171\u4EAB\u548C\u672C\u5730\u5185\u5B58\u8303\u56F4\u3002\u4E3A\u4E86\u652F\u6301 TensorCores\uFF0C\u6DFB\u52A0\u53E6\u5916\u4E09\u4E2A\u7279\u6B8A\u7684\u5185\u5B58\u8303\u56F4\uFF1A",(0,r.jsx)(e.code,{children:"wmma.matrix_a"}),", ",(0,r.jsx)(e.code,{children:"wmma.matrix_b"})," \u548C ",(0,r.jsx)(e.code,{children:"wmma.accumulator"}),"\u3002\u5728\u786C\u4EF6\u4E0A\uFF0C\u6240\u6709\u7247\u6BB5\u8303\u56F4\u90FD\u5B58\u50A8\u5728\u82AF\u7247\u4E0A\u5BC4\u5B58\u5668\u7EA7\u522B\uFF0C\u4E0E\u672C\u5730\u5185\u5B58\u76F8\u540C\u3002"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# \u6307\u5B9A\u5185\u5B58\u5C42\u6B21\u7ED3\u6784\nAS = s.cache_read(Apad, "shared", [Conv])\nWS = s.cache_read(W, "shared", [Conv])\nAF = s.cache_read(AS, "wmma.matrix_a", [Conv])\nWF = s.cache_read(WS, "wmma.matrix_b", [Conv])\nConvF = s.cache_write(Conv, "wmma.accumulator")\n'})}),"\n",(0,r.jsx)(e.h2,{id:"\u5B9A\u4E49\u5F20\u91CF\u5185\u8054\u51FD\u6570",children:"\u5B9A\u4E49\u5F20\u91CF\u5185\u8054\u51FD\u6570"}),"\n",(0,r.jsx)(e.p,{children:"\u5B9E\u9645\u4E0A\uFF0CTensorCore \u662F\u4E00\u79CD\u7279\u6B8A\u7684\u786C\u4EF6\u64CD\u4F5C\u3002\u56E0\u6B64\uFF0C\u53EF\u4EE5\u53EA\u7528 tensorize \u5C06\u4E00\u4E2A\u8BA1\u7B97\u5355\u5143\u66FF\u6362\u4E3A TensorCore \u6307\u4EE4\u3002\u9996\u5148\u9700\u8981\u5B9A\u4E49\u5F20\u91CF\u5185\u8054\u51FD\u6570\u3002"}),"\n",(0,r.jsxs)(e.p,{children:["TensorCore \u4E2D\u6709\u56DB\u4E2A\u57FA\u672C\u64CD\u4F5C\uFF1A",(0,r.jsx)(e.code,{children:"fill_fragment"}),"\uFF0C ",(0,r.jsx)(e.code,{children:"load_matrix"}),"\uFF0C ",(0,r.jsx)(e.code,{children:"mma_sync"})," \u548C ",(0,r.jsx)(e.code,{children:"store_matrix"}),"\u3002\u7531\u4E8E ",(0,r.jsx)(e.code,{children:"fill_fragment"})," \u548C ",(0,r.jsx)(e.code,{children:"mma_sync"})," \u90FD\u7528\u4E8E\u77E9\u9635\u4E58\u6CD5\uFF0C\u6240\u4EE5\u53EF\u4EE5\u53EA\u5199\u4EE5\u4E0B\u4E09\u4E2A\u5185\u8054\u51FD\u6570\u3002"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'def intrin_wmma_load_matrix(scope):\n    n = 16\n    A = te.placeholder((n, n), name="A", dtype="float16")\n    BA = tvm.tir.decl_buffer(A.shape, A.dtype, scope="shared", data_alignment=32, offset_factor=256)\n    C = te.compute((n, n), lambda i, j: A[i, j], name="C")\n    BC = tvm.tir.decl_buffer(C.shape, C.dtype, scope=scope, data_alignment=32, offset_factor=256)\n\n    def intrin_func(ins, outs):\n        ib = tvm.tir.ir_builder.create()\n\n        BA = ins[0]\n        BC = outs[0]\n        ib.emit(\n            tvm.tir.call_intrin(\n                "handle",\n                "tir.tvm_load_matrix_sync",\n                BC.data,\n                n,\n                n,\n                n,\n                BC.elem_offset // 256,\n                BA.access_ptr("r"),\n                n,\n                "row_major",\n            )\n        )\n        return ib.get()\n\n    return te.decl_tensor_intrin(C.op, intrin_func, binds={A: BA, C: BC})\n\ndef intrin_wmma_gemm():\n    n = 16\n    A = te.placeholder((n, n), name="A", dtype="float16")\n    B = te.placeholder((n, n), name="B", dtype="float16")\n    k = te.reduce_axis((0, n), name="k")\n    C = te.compute(\n        (n, n),\n        lambda ii, jj: te.sum(A[ii, k].astype("float") * B[k, jj].astype("float"), axis=k),\n        name="C",\n    )\n    BA = tvm.tir.decl_buffer(\n        A.shape, A.dtype, name="BA", scope="wmma.matrix_a", data_alignment=32, offset_factor=256\n    )\n    BB = tvm.tir.decl_buffer(\n        B.shape, B.dtype, name="BB", scope="wmma.matrix_b", data_alignment=32, offset_factor=256\n    )\n    BC = tvm.tir.decl_buffer(\n        C.shape, C.dtype, name="BC", scope="wmma.accumulator", data_alignment=32, offset_factor=256\n    )\n\n    def intrin_func(ins, outs):\n        BA, BB = ins\n        (BC,) = outs\n\n        def init():\n            ib = tvm.tir.ir_builder.create()\n            ib.emit(\n                tvm.tir.call_intrin(\n                    "handle", "tir.tvm_fill_fragment", BC.data, n, n, n, BC.elem_offset // 256, 0.0\n                )\n            )\n            return ib.get()\n\n        def update():\n            ib = tvm.tir.ir_builder.create()\n            ib.emit(\n                tvm.tir.call_intrin(\n                    "handle",\n                    "tir.tvm_mma_sync",\n                    BC.data,\n                    BC.elem_offset // 256,\n                    BA.data,\n                    BA.elem_offset // 256,\n                    BB.data,\n                    BB.elem_offset // 256,\n                    BC.data,\n                    BC.elem_offset // 256,\n                )\n            )\n            return ib.get()\n\n        return update(), init(), update()\n\n    return te.decl_tensor_intrin(C.op, intrin_func, binds={A: BA, B: BB, C: BC})\n\ndef intrin_wmma_store_matrix():\n    n = 16\n    A = te.placeholder((n, n), name="A", dtype="float32")\n    BA = tvm.tir.decl_buffer(\n        A.shape, A.dtype, scope="wmma.accumulator", data_alignment=32, offset_factor=256\n    )\n    C = te.compute((n, n), lambda i, j: A[i, j], name="C")\n    BC = tvm.tir.decl_buffer(C.shape, C.dtype, scope="global", data_alignment=32, offset_factor=256)\n\n    def intrin_func(ins, outs):\n        ib = tvm.tir.ir_builder.create()\n        BA = ins[0]\n        BC = outs[0]\n        ib.emit(\n            tvm.tir.call_intrin(\n                "handle",\n                "tir.tvm_store_matrix_sync",\n                BA.data,\n                n,\n                n,\n                n,\n                BA.elem_offset // 256,\n                BC.access_ptr("w"),\n                n,\n                "row_major",\n            )\n        )\n        return ib.get()\n\n    return te.decl_tensor_intrin(C.op, intrin_func, binds={A: BA, C: BC})\n'})}),"\n",(0,r.jsx)(e.h2,{id:"\u8C03\u5EA6\u8BA1\u7B97",children:"\u8C03\u5EA6\u8BA1\u7B97"}),"\n",(0,r.jsxs)(e.p,{children:["\u8981\u5728 TVM \u4E2D\u4F7F\u7528 TensorCores\uFF0C\u5FC5\u987B\u5C06\u8BA1\u7B97\u8C03\u5EA6\u5230\u7279\u5B9A\u7684\u7ED3\u6784\u4E2D\uFF0C\u4ECE\u800C\u5339\u914D\u5F20\u91CF\u5185\u8054\u51FD\u6570\u3002\u548C\u4F20\u7EDF\u7684 GPU \u7A0B\u5E8F\u4E00\u6837\uFF0C\u4E5F\u53EF\u4EE5\u7528\u5171\u4EAB\u5185\u5B58\u6765\u63D0\u5347\u901F\u5EA6\u3002\u5982\u679C\u5BF9\u5206\u5757\u548C\u5171\u4EAB\u5185\u5B58\u6709\u4EFB\u4F55\u7591\u95EE\uFF0C\u8BF7\u53C2\u9605 ",(0,r.jsx)(e.a,{href:"gpu_conv",children:"\u5982\u4F55\u5728 GPU \u4E0A\u4F18\u5316\u5377\u79EF"}),"\u3002"]}),"\n",(0,r.jsx)(e.p,{children:"\u5728\u8FD9\u4E2A\u4F8B\u5B50\u4E2D\uFF0C\u6BCF\u4E2A\u5757\u5305\u542B 2x4 \u4E2A warp\uFF0C\u6BCF\u4E2A warp \u8C03\u7528 4x2 TensorCore \u6307\u4EE4\u3002\u56E0\u6B64\uFF0C\u6BCF\u4E2A warp \u7684\u8F93\u51FA shape \u4E3A 64x32\uFF0C\u6BCF\u4E2A\u5757\u8F93\u51FA 128x128 \u4E2A titles\u3002\u7531\u4E8E\u5171\u4EAB\u5185\u5B58\u7A7A\u95F4\u7684\u9650\u5236\uFF0C\u4E00\u6B21\u53EA\u52A0\u8F7D 2 \u4E2A\u5757\uFF082x128x128 tiles\uFF09\u3002"}),"\n",(0,r.jsxs)(e.admonition,{type:"note",children:[(0,r.jsx)(e.p,{children:(0,r.jsx)(e.em,{children:"Warp \u7EA7\u64CD\u4F5C"})}),(0,r.jsx)(e.p,{children:"\u6240\u6709 TensorCore \u6307\u4EE4\u90FD\u662F warp \u7EA7\u6307\u4EE4\uFF0C\u8FD9\u610F\u5473\u7740\u4E00\u4E2A warp \u4E2D\u7684\u6240\u6709 32 \u4E2A\u7EBF\u7A0B\u5E94\u8BE5\u540C\u65F6\u6267\u884C\u6B64\u6307\u4EE4\u3002"}),(0,r.jsx)(e.p,{children:"threadIdx.x extent=32 \u662F\u89E3\u51B3\u6B64\u95EE\u9898\u7684\u6700\u7B80\u5355\u65B9\u6CD5\u4E4B\u4E00\u3002\u9664\u4E86\u90A3\u4E9B\u76F4\u63A5\u6216\u95F4\u63A5\u5305\u542B TensorCore \u5185\u8054\u51FD\u6570\u7684\u5FAA\u73AF\uFF0C\u53EF\u4EE5\u5C06 threadIdx.x \u7ED1\u5B9A\u5230\u4EFB\u4F55\u5FAA\u73AF\u3002"}),(0,r.jsx)(e.p,{children:"\u8FD9\u5E76\u4E0D\u662F\u552F\u4E00\u7684\u89E3\u51B3\u65B9\u6848\uFF0C\u552F\u4E00\u5E94\u8BE5\u505A\u7684\u5C31\u662F\u786E\u4FDD\u4E00\u4E2A warp \u4E2D\u7684\u6240\u6709\u7EBF\u7A0B\u90FD\u53EF\u4EE5\u540C\u65F6\u8C03\u7528 TensorCore\u3002"})]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# \u5B9A\u4E49 tile \u5927\u5C0F\nblock_row_warps = 4\nblock_col_warps = 2\nwarp_row_tiles = 2\nwarp_col_tiles = 4\nwarp_size = 32\nchunk = 2\n\nblock_x = te.thread_axis("blockIdx.x")\nblock_y = te.thread_axis("blockIdx.y")\nblock_z = te.thread_axis("blockIdx.z")\nthread_x = te.thread_axis("threadIdx.x")\nthread_y = te.thread_axis("threadIdx.y")\nthread_z = te.thread_axis("threadIdx.z")\n\nnc, hc, wc, oc, nnc, ooc = Conv.op.axis\nblock_k = s[Conv].fuse(hc, wc)\ns[Conv].bind(block_k, block_z)\nnc, nci = s[Conv].split(nc, factor=warp_row_tiles)\nblock_i, nc = s[Conv].split(nc, factor=block_row_warps)\noc, oci = s[Conv].split(oc, factor=warp_col_tiles)\nblock_j, oc = s[Conv].split(oc, factor=block_col_warps)\ns[Conv].reorder(block_k, block_i, block_j, nc, oc, nci, oci, nnc, ooc)\ns[Conv].bind(block_i, block_x)\ns[Conv].bind(block_j, block_y)\ns[Conv].bind(nc, thread_y)\ns[Conv].bind(oc, thread_z)\n\n# \u8C03\u5EA6\u672C\u5730\u8BA1\u7B97\ns[ConvF].compute_at(s[Conv], oc)\nn, h, w, o, nnf, oof = ConvF.op.axis\nko, ki = s[ConvF].split(ic, factor=chunk)\ns[ConvF].reorder(ko, kh, ki, kw, n, o, nnf, oof, ii)\n\n# \u5C06\u4E2D\u95F4\u8BA1\u7B97\u79FB\u52A8\u5230\u6BCF\u4E2A\u8F93\u51FA\u8BA1\u7B97\u5757\u4E2D\ns[AF].compute_at(s[ConvF], kw)\ns[WF].compute_at(s[ConvF], kw)\n\n# A \u7684\u5171\u4EAB\u5185\u5B58\u8C03\u5EA6\ns[AS].compute_at(s[ConvF], kh)\nn, h, w, i, nn, ii = AS.op.axis\ntx, xo = s[AS].split(n, nparts=block_row_warps)\nty, yo = s[AS].split(xo, nparts=block_col_warps)\nt = s[AS].fuse(nn, ii)\nto, ti = s[AS].split(t, factor=warp_size)\ns[AS].bind(tx, thread_y)\ns[AS].bind(ty, thread_z)\ns[AS].bind(ti, thread_x)\n\n# W \u7684\u5171\u4EAB\u5185\u5B58\u8C03\u5EA6\ns[WS].compute_at(s[ConvF], kh)\nkh, kw, ic, o, ii, oo = WS.op.axis\ntx, xo = s[WS].split(o, nparts=block_row_warps)\nty, yo = s[WS].split(xo, nparts=block_col_warps)\nt = s[WS].fuse(ii, oo)\nto, ti = s[WS].split(t, nparts=warp_size)\ns[WS].bind(tx, thread_y)\ns[WS].bind(ty, thread_z)\ns[WS].bind(to, thread_x)\ns[WS].vectorize(ti)\nprint(tvm.lower(s, [A, W, Conv], simple_mode=True))\n'})}),"\n",(0,r.jsx)(e.p,{children:"\u8F93\u51FA\u7ED3\u679C\uFF1A"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:'@main = primfn(A_1: handle, W_1: handle, Conv_1: handle) -> ()\n  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}\n  buffers = {A: Buffer(A_2: Pointer(float16), float16, [12845056], []),\n             W: Buffer(W_2: Pointer(float16), float16, [1179648], []),\n             Conv: Buffer(Conv_2: Pointer(float32), float32, [25690112], [])}\n  buffer_map = {A_1: A, W_1: W, Conv_1: Conv}\n  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float16, [16, 14, 14, 16, 16, 16], []), W_1: W_3: Buffer(W_2, float16, [3, 3, 16, 32, 16, 16], []), Conv_1: Conv_3: Buffer(Conv_2, float32, [16, 14, 14, 32, 16, 16], [])} {\n  attr [IterVar(blockIdx.z: int32, (nullptr), "ThreadIndex", "blockIdx.z")] "thread_extent" = 196;\n  allocate(Conv.wmma.accumulator: Pointer(wmma.accumulator float32), float32, [2048]), storage_scope = wmma.accumulator;\n  allocate(Apad.shared: Pointer(shared float16), float16, [12288]), storage_scope = shared;\n  allocate(W.shared: Pointer(shared float16), float16, [12288]), storage_scope = shared;\n  allocate(Apad.shared.wmma.matrix_a: Pointer(wmma.matrix_a float16), float16, [512]), storage_scope = wmma.matrix_a;\n  allocate(W.shared.wmma.matrix_b: Pointer(wmma.matrix_b float16), float16, [1024]), storage_scope = wmma.matrix_b;\n  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 2;\n  attr [IterVar(blockIdx.y: int32, (nullptr), "ThreadIndex", "blockIdx.y")] "thread_extent" = 4;\n  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 4;\n  attr [IterVar(threadIdx.z: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 2 {\n    for (n.c.init: int32, 0, 2) {\n      for (o.c.init: int32, 0, 4) {\n        for (nn.c.init: int32, 0, 16) {\n          for (oo.c.init: int32, 0, 16) {\n            Conv.wmma.accumulator_1: Buffer(Conv.wmma.accumulator, float32, [2048], [], scope="wmma.accumulator")[((((n.c.init*1024) + (o.c.init*256)) + (nn.c.init*16)) + oo.c.init)] = 0f32\n          }\n        }\n      }\n    }\n    for (ic.outer: int32, 0, 8) {\n      for (kh: int32, 0, 3) {\n        for (ax2: int32, 0, 3) {\n          for (ax3: int32, 0, 2) {\n            for (ax4.ax5.fused.outer: int32, 0, 8) {\n              let cse_var_2: int32 = (ax3*256)\n              let cse_var_1: int32 = (ax4.ax5.fused.outer*32)\n              attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;\n              Apad.shared_1: Buffer(Apad.shared, float16, [12288], [], scope="shared")[((((((threadIdx.y*3072) + (threadIdx.z*1536)) + (ax2*512)) + cse_var_2) + cse_var_1) + threadIdx.x)] = @tir.if_then_else(((((1 <= (floordiv(blockIdx.z, 14) + kh)) && ((floordiv(blockIdx.z, 14) + kh) < 15)) && (1 <= (ax2 + floormod(blockIdx.z, 14)))) && ((ax2 + floormod(blockIdx.z, 14)) < 15)), A[(((((((((((blockIdx.x*6422528) + (threadIdx.y*1605632)) + (threadIdx.z*802816)) + (kh*57344)) + (blockIdx.z*4096)) + (ax2*4096)) + (ic.outer*512)) + cse_var_2) + cse_var_1) + threadIdx.x) - 61440)], 0f16, dtype=float16)\n            }\n          }\n        }\n        for (ax1: int32, 0, 3) {\n          for (ax2_1: int32, 0, 2) {\n            attr [IterVar(threadIdx.x, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;\n            W.shared_1: Buffer(W.shared, float16, [12288], [], scope="shared")[ramp((((((ax1*4096) + (ax2_1*2048)) + (threadIdx.y*512)) + (threadIdx.z*256)) + (threadIdx.x*8)), 1, 8)] = W[ramp(((((((((kh*393216) + (ax1*131072)) + (ic.outer*16384)) + (ax2_1*8192)) + (blockIdx.y*2048)) + (threadIdx.y*512)) + (threadIdx.z*256)) + (threadIdx.x*8)), 1, 8)]\n          }\n        }\n        for (ic.inner: int32, 0, 2) {\n          for (kw: int32, 0, 3) {\n            for (ax0: int32, 0, 2) {\n              for (ax4: int32, 0, 16) {\n                for (ax5: int32, 0, 16) {\n                  let cse_var_3: int32 = (ax4*16)\n                  Apad.shared.wmma.matrix_a_1: Buffer(Apad.shared.wmma.matrix_a, float16, [512], [], scope="wmma.matrix_a")[(((ax0*256) + cse_var_3) + ax5)] = Apad.shared_1[((((((threadIdx.y*3072) + (ax0*1536)) + (kw*512)) + (ic.inner*256)) + cse_var_3) + ax5)]\n                }\n              }\n            }\n            for (ax3_1: int32, 0, 4) {\n              for (ax4_1: int32, 0, 16) {\n                for (ax5_1: int32, 0, 16) {\n                  let cse_var_5: int32 = (ax3_1*256)\n                  let cse_var_4: int32 = (ax4_1*16)\n                  W.shared.wmma.matrix_b_1: Buffer(W.shared.wmma.matrix_b, float16, [1024], [], scope="wmma.matrix_b")[((cse_var_5 + cse_var_4) + ax5_1)] = W.shared_1[((((((kw*4096) + (ic.inner*2048)) + (threadIdx.z*1024)) + cse_var_5) + cse_var_4) + ax5_1)]\n                }\n              }\n            }\n            for (n.c: int32, 0, 2) {\n              for (o.c: int32, 0, 4) {\n                for (nn.c: int32, 0, 16) {\n                  for (oo.c: int32, 0, 16) {\n                    for (ii: int32, 0, 16) {\n                      let cse_var_8: int32 = (o.c*256)\n                      let cse_var_7: int32 = (nn.c*16)\n                      let cse_var_6: int32 = ((((n.c*1024) + cse_var_8) + cse_var_7) + oo.c)\n                      Conv.wmma.accumulator_1[cse_var_6] = (Conv.wmma.accumulator_1[cse_var_6] + (cast(float32, Apad.shared.wmma.matrix_a_1[(((n.c*256) + cse_var_7) + ii)])*cast(float32, W.shared.wmma.matrix_b_1[((cse_var_8 + (ii*16)) + oo.c)])))\n                    }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n    for (n.inner: int32, 0, 2) {\n      for (o.inner: int32, 0, 4) {\n        for (nn: int32, 0, 16) {\n          for (oo: int32, 0, 16) {\n            let cse_var_10: int32 = (o.inner*256)\n            let cse_var_9: int32 = (nn*16)\n            Conv[(((((((((blockIdx.x*12845056) + (threadIdx.y*3211264)) + (n.inner*1605632)) + (blockIdx.z*8192)) + (blockIdx.y*2048)) + (threadIdx.z*1024)) + cse_var_10) + cse_var_9) + oo)] = Conv.wmma.accumulator_1[((((n.inner*1024) + cse_var_10) + cse_var_9) + oo)]\n          }\n        }\n      }\n    }\n  }\n}\n'})}),"\n",(0,r.jsx)(e.h2,{id:"\u5C06\u8BA1\u7B97\u964D\u7EA7\u4E3A\u5185\u8054\u51FD\u6570",children:"\u5C06\u8BA1\u7B97\u964D\u7EA7\u4E3A\u5185\u8054\u51FD\u6570"}),"\n",(0,r.jsx)(e.p,{children:"\u6700\u540E\u4E00\u4E2A\u9636\u6BB5\u5C06\u8BA1\u7B97\u5FAA\u73AF\u964D\u7EA7\u5230 TensorCore \u786C\u4EF6\u5185\u8054\u51FD\u6570\uFF0C\u8FD9\u662F\u901A\u8FC7\u5C06 2D \u5377\u79EF\u6620\u5C04\u5230\u5F20\u91CF\u5185\u8054\u51FD\u6570\u5B9E\u73B0\u7684\u3002"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'s[AF].tensorize(AF.op.axis[-2], intrin_wmma_load_matrix("wmma.matrix_a"))\ns[WF].tensorize(WF.op.axis[-2], intrin_wmma_load_matrix("wmma.matrix_b"))\ns[Conv].tensorize(nnc, intrin_wmma_store_matrix())\ns[ConvF].tensorize(nnf, intrin_wmma_gemm())\nprint(tvm.lower(s, [A, W, Conv], simple_mode=True))\n'})}),"\n",(0,r.jsx)(e.p,{children:"\u8F93\u51FA\u7ED3\u679C\uFF1A"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:'@main = primfn(A_1: handle, W_1: handle, Conv_1: handle) -> ()\n  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}\n  buffers = {A: Buffer(A_2: Pointer(float16), float16, [12845056], []),\n             W: Buffer(W_2: Pointer(float16), float16, [1179648], []),\n             Conv: Buffer(Conv_2: Pointer(float32), float32, [25690112], [])}\n  buffer_map = {A_1: A, W_1: W, Conv_1: Conv}\n  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float16, [16, 14, 14, 16, 16, 16], []), W_1: W_3: Buffer(W_2, float16, [3, 3, 16, 32, 16, 16], []), Conv_1: Conv_3: Buffer(Conv_2, float32, [16, 14, 14, 32, 16, 16], [])} {\n  attr [IterVar(blockIdx.z: int32, (nullptr), "ThreadIndex", "blockIdx.z")] "thread_extent" = 196;\n  allocate(Conv.wmma.accumulator: Pointer(wmma.accumulator float32), float32, [2048]), storage_scope = wmma.accumulator;\n  allocate(Apad.shared: Pointer(shared float16), float16, [12288]), storage_scope = shared;\n  allocate(W.shared: Pointer(shared float16), float16, [12288]), storage_scope = shared;\n  allocate(Apad.shared.wmma.matrix_a: Pointer(wmma.matrix_a float16), float16, [512]), storage_scope = wmma.matrix_a;\n  allocate(W.shared.wmma.matrix_b: Pointer(wmma.matrix_b float16), float16, [1024]), storage_scope = wmma.matrix_b;\n  attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 2;\n  attr [IterVar(blockIdx.y: int32, (nullptr), "ThreadIndex", "blockIdx.y")] "thread_extent" = 4;\n  attr [IterVar(threadIdx.y: int32, (nullptr), "ThreadIndex", "threadIdx.y")] "thread_extent" = 4;\n  attr [IterVar(threadIdx.z: int32, (nullptr), "ThreadIndex", "threadIdx.z")] "thread_extent" = 2 {\n    for (n.c.init: int32, 0, 2) {\n      for (o.c.init: int32, 0, 4) {\n        @tir.tvm_fill_fragment(Conv.wmma.accumulator, 16, 16, 16, ((n.c.init*4) + o.c.init), 0f32, dtype=handle)\n      }\n    }\n    for (ic.outer: int32, 0, 8) {\n      for (kh: int32, 0, 3) {\n        for (ax2: int32, 0, 3) {\n          for (ax3: int32, 0, 2) {\n            for (ax4.ax5.fused.outer: int32, 0, 8) {\n              let cse_var_2: int32 = (ax3*256)\n              let cse_var_1: int32 = (ax4.ax5.fused.outer*32)\n              attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;\n              Apad.shared_1: Buffer(Apad.shared, float16, [12288], [], scope="shared")[((((((threadIdx.y*3072) + (threadIdx.z*1536)) + (ax2*512)) + cse_var_2) + cse_var_1) + threadIdx.x)] = @tir.if_then_else(((((1 <= (floordiv(blockIdx.z, 14) + kh)) && ((floordiv(blockIdx.z, 14) + kh) < 15)) && (1 <= (ax2 + floormod(blockIdx.z, 14)))) && ((ax2 + floormod(blockIdx.z, 14)) < 15)), A[(((((((((((blockIdx.x*6422528) + (threadIdx.y*1605632)) + (threadIdx.z*802816)) + (kh*57344)) + (blockIdx.z*4096)) + (ax2*4096)) + (ic.outer*512)) + cse_var_2) + cse_var_1) + threadIdx.x) - 61440)], 0f16, dtype=float16)\n            }\n          }\n        }\n        for (ax1: int32, 0, 3) {\n          for (ax2_1: int32, 0, 2) {\n            attr [IterVar(threadIdx.x, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 32;\n            W.shared_1: Buffer(W.shared, float16, [12288], [], scope="shared")[ramp((((((ax1*4096) + (ax2_1*2048)) + (threadIdx.y*512)) + (threadIdx.z*256)) + (threadIdx.x*8)), 1, 8)] = W[ramp(((((((((kh*393216) + (ax1*131072)) + (ic.outer*16384)) + (ax2_1*8192)) + (blockIdx.y*2048)) + (threadIdx.y*512)) + (threadIdx.z*256)) + (threadIdx.x*8)), 1, 8)]\n          }\n        }\n        for (ic.inner: int32, 0, 2) {\n          for (kw: int32, 0, 3) {\n            for (ax0: int32, 0, 2) {\n              @tir.tvm_load_matrix_sync(Apad.shared.wmma.matrix_a, 16, 16, 16, ax0, @tir.tvm_access_ptr(@tir.type_annotation(, dtype=float16), Apad.shared, ((((threadIdx.y*3072) + (ax0*1536)) + (kw*512)) + (ic.inner*256)), 256, 1, dtype=handle), 16, "row_major", dtype=handle)\n            }\n            for (ax3_1: int32, 0, 4) {\n              @tir.tvm_load_matrix_sync(W.shared.wmma.matrix_b, 16, 16, 16, ax3_1, @tir.tvm_access_ptr(@tir.type_annotation(, dtype=float16), W.shared, ((((kw*4096) + (ic.inner*2048)) + (threadIdx.z*1024)) + (ax3_1*256)), 256, 1, dtype=handle), 16, "row_major", dtype=handle)\n            }\n            for (n.c: int32, 0, 2) {\n              for (o.c: int32, 0, 4) {\n                let cse_var_3: int32 = ((n.c*4) + o.c)\n                @tir.tvm_mma_sync(Conv.wmma.accumulator, cse_var_3, Apad.shared.wmma.matrix_a, n.c, W.shared.wmma.matrix_b, o.c, Conv.wmma.accumulator, cse_var_3, dtype=handle)\n              }\n            }\n          }\n        }\n      }\n    }\n    for (n.inner: int32, 0, 2) {\n      for (o.inner: int32, 0, 4) {\n        @tir.tvm_store_matrix_sync(Conv.wmma.accumulator, 16, 16, 16, ((n.inner*4) + o.inner), @tir.tvm_access_ptr(@tir.type_annotation(, dtype=float32), Conv_2, (((((((blockIdx.x*12845056) + (threadIdx.y*3211264)) + (n.inner*1605632)) + (blockIdx.z*8192)) + (blockIdx.y*2048)) + (threadIdx.z*1024)) + (o.inner*256)), 256, 2, dtype=handle), 16, "row_major", dtype=handle)\n      }\n    }\n  }\n}\n'})}),"\n",(0,r.jsx)(e.h2,{id:"\u751F\u6210-cuda-\u5185\u6838",children:"\u751F\u6210 CUDA \u5185\u6838"}),"\n",(0,r.jsx)(e.p,{children:"\u6700\u540E\u4F7F\u7528 TVM \u751F\u6210\u548C\u7F16\u8BD1 CUDA \u5185\u6838\uFF0C\u5E76\u8BC4\u4F30\u5377\u79EF\u7684\u5EF6\u8FDF\u3002\u7531\u4E8E TensorCores \u4EC5\u652F\u6301 Compute Capability 7.0 \u6216\u66F4\u9AD8\u7248\u672C\u7684 NVIDIA GPU\uFF0C\u56E0\u6B64\u53EF\u80FD\u65E0\u6CD5\u5728\u6784\u5EFA\u670D\u52A1\u5668\u4E0A\u8FD0\u884C\u3002"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:'dev = tvm.cuda(0)\nif nvcc.have_tensorcore(dev.compute_version):\n    with tvm.transform.PassContext(config={"tir.UnrollLoop": {"auto_max_step": 16}}):\n        func = tvm.build(s, [A, W, Conv], "cuda")\n    a_np = np.random.uniform(size=data_shape).astype(A.dtype)\n    w_np = np.random.uniform(size=kernel_shape).astype(W.dtype)\n    a = tvm.nd.array(a_np, dev)\n    w = tvm.nd.array(w_np, dev)\n    c = tvm.nd.array(np.zeros(output_shape, dtype=Conv.dtype), dev)\n    evaluator = func.time_evaluator(func.entry_name, dev, number=10)\n    print("conv2d with tensor core: %f ms" % (evaluator(a, w, c).mean * 1e3))\n'})}),"\n",(0,r.jsx)(e.p,{children:"\u8F93\u51FA\u7ED3\u679C\uFF1A"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"conv2d with tensor core: 6.835711 ms\n"})}),"\n",(0,r.jsx)(e.h2,{id:"\u603B\u7ED3",children:"\u603B\u7ED3"}),"\n",(0,r.jsx)(e.p,{children:"\u672C\u6559\u7A0B\u6F14\u793A\u5982\u4F55\u7528 TVM \u8C03\u5EA6\u539F\u8BED\u5728\u7279\u5B9A GPU \u4E0A\u8C03\u7528 TensorCore\u3002"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.a,{href:"https://tvm.apache.org/docs/_downloads/7372db5919b5619bc34fde3434862bca/opt_conv_tensorcore.py",children:"\u4E0B\u8F7D Python \u6E90\u4EE3\u7801\uFF1Aopt_conv_tensorcore.py"})}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.a,{href:"https://tvm.apache.org/docs/_downloads/7455981870c23c8c76482dedf33d8a42/opt_conv_tensorcore.ipynb",children:"\u4E0B\u8F7D Jupyter Notebook\uFF1Aopt_conv_tensorcore.ipynb"})})]})}function _(n={}){let{wrapper:e}={...(0,o.a)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(l,{...n})}):l(n)}},21494:function(n,e,t){t.d(e,{Z:function(){return d},a:function(){return i}});var a=t(39546);let r={},o=a.createContext(r);function i(n){let e=a.useContext(o);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function d(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:i(n.components),a.createElement(o.Provider,{value:e},n.children)}}}]);