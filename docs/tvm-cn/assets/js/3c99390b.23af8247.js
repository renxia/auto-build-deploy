"use strict";(self.webpackChunktvm_cn=self.webpackChunktvm_cn||[]).push([["77111"],{38420:function(e,o,n){n.r(o),n.d(o,{default:()=>p,frontMatter:()=>i,metadata:()=>r,assets:()=>a,toc:()=>d,contentTitle:()=>c});var r=JSON.parse('{"id":"how_to/deploy/deploy_models/compile_od","title":"\u7F16\u8BD1 PyTorch \u76EE\u6807\u68C0\u6D4B\u6A21\u578B","description":"\u5355\u51FB \u6B64\u5904 \u4E0B\u8F7D\u5B8C\u6574\u7684\u793A\u4F8B\u4EE3\u7801","source":"@site/versioned_docs/version-0.12.0/how_to/deploy/deploy_models/04-compile_od.md","sourceDirName":"how_to/deploy/deploy_models","slug":"/how_to/deploy/deploy_models/compile_od","permalink":"/docs/tvm-cn/docs/0.12.0/how_to/deploy/deploy_models/compile_od","draft":false,"unlisted":false,"editUrl":"https://github.com/hyperai/tvm-cn/edit/master/versioned_docs/version-0.12.0/how_to/deploy/deploy_models/04-compile_od.md","tags":[],"version":"0.12.0","lastUpdatedBy":"sparanoid","lastUpdatedAt":1744717810000,"sidebarPosition":4,"frontMatter":{"title":"\u7F16\u8BD1 PyTorch \u76EE\u6807\u68C0\u6D4B\u6A21\u578B"},"sidebar":"tutorialSidebar","previous":{"title":"\u5728\u6811\u8393\u6D3E\u4E0A\u90E8\u7F72\u9884\u8BAD\u7EC3\u6A21\u578B","permalink":"/docs/tvm-cn/docs/0.12.0/how_to/deploy/deploy_models/deploy_pi"},"next":{"title":"\u4F7F\u7528 TVM \u90E8\u7F72\u6846\u67B6\u9884\u91CF\u5316\u6A21\u578B","permalink":"/docs/tvm-cn/docs/0.12.0/how_to/deploy/deploy_models/deploy_prequan"}}'),t=n("74132"),s=n("21494");let i={title:"\u7F16\u8BD1 PyTorch \u76EE\u6807\u68C0\u6D4B\u6A21\u578B"},c="\u7F16\u8BD1 PyTorch \u76EE\u6807\u68C0\u6D4B\u6A21\u578B",a={},d=[{value:"\u4ECE TorchVision \u52A0\u8F7D\u9884\u8BAD\u7EC3\u7684 MaskRCNN \u5E76\u8FDB\u884C\u8DDF\u8E2A",id:"\u4ECE-torchvision-\u52A0\u8F7D\u9884\u8BAD\u7EC3\u7684-maskrcnn-\u5E76\u8FDB\u884C\u8DDF\u8E2A",level:2},{value:"\u4E0B\u8F7D\u6D4B\u8BD5\u56FE\u50CF\u5E76\u8FDB\u884C\u9884\u5904\u7406",id:"\u4E0B\u8F7D\u6D4B\u8BD5\u56FE\u50CF\u5E76\u8FDB\u884C\u9884\u5904\u7406",level:2},{value:"\u5C06\u8BA1\u7B97\u56FE\u5BFC\u5165 Relay",id:"\u5C06\u8BA1\u7B97\u56FE\u5BFC\u5165-relay",level:2},{value:"\u4F7F\u7528 Relay VM \u7F16\u8BD1",id:"\u4F7F\u7528-relay-vm-\u7F16\u8BD1",level:2},{value:"\u4F7F\u7528 Relay VM \u8FDB\u884C\u63A8\u7406",id:"\u4F7F\u7528-relay-vm-\u8FDB\u884C\u63A8\u7406",level:2},{value:"\u83B7\u53D6 score \u5927\u4E8E 0.9 \u7684 box",id:"\u83B7\u53D6-score-\u5927\u4E8E-09-\u7684-box",level:2}];function l(e){let o={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",strong:"strong",...(0,s.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(o.header,{children:(0,t.jsx)(o.h1,{id:"\u7F16\u8BD1-pytorch-\u76EE\u6807\u68C0\u6D4B\u6A21\u578B",children:"\u7F16\u8BD1 PyTorch \u76EE\u6807\u68C0\u6D4B\u6A21\u578B"})}),"\n",(0,t.jsx)(o.admonition,{type:"note",children:(0,t.jsxs)(o.p,{children:["\u5355\u51FB ",(0,t.jsx)(o.a,{href:"https://tvm.apache.org/docs/how_to/deploy_models/deploy_object_detection_pytorch.html#sphx-glr-download-how-to-deploy-models-deploy-object-detection-pytorch-py",children:"\u6B64\u5904"})," \u4E0B\u8F7D\u5B8C\u6574\u7684\u793A\u4F8B\u4EE3\u7801"]})}),"\n",(0,t.jsx)(o.p,{children:"\u672C\u6587\u4ECB\u7ECD\u5982\u4F55\u7528 Relay VM \u90E8\u7F72 PyTorch \u76EE\u6807\u68C0\u6D4B\u6A21\u578B\u3002"}),"\n",(0,t.jsx)(o.p,{children:"\u9996\u5148\u5E94\u5B89\u88C5 PyTorch\u3002\u6B64\u5916\uFF0C\u8FD8\u5E94\u5B89\u88C5 TorchVision\uFF0C\u5E76\u5C06\u5176\u4F5C\u4E3A\u6A21\u578B\u5408\u96C6\uFF08model zoo\uFF09\u3002"}),"\n",(0,t.jsx)(o.p,{children:"\u53EF\u901A\u8FC7 pip \u5FEB\u901F\u5B89\u88C5\uFF1A"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-bash",children:"pip install torch==1.7.0\npip install torchvision==0.8.1\n"})}),"\n",(0,t.jsxs)(o.p,{children:["\u6216\u53C2\u8003\u5B98\u7F51\uFF1A",(0,t.jsx)(o.a,{href:"https://pytorch.org/get-started/locally/",children:"https://pytorch.org/get-started/locally/"})]}),"\n",(0,t.jsx)(o.p,{children:"PyTorch \u7248\u672C\u5E94\u8BE5\u548C TorchVision \u7248\u672C\u517C\u5BB9\u3002"}),"\n",(0,t.jsx)(o.p,{children:"\u76EE\u524D TVM \u652F\u6301 PyTorch 1.7 \u548C 1.4\uFF0C\u5176\u4ED6\u7248\u672C\u53EF\u80FD\u4E0D\u7A33\u5B9A\u3002"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-python",children:"import tvm\nfrom tvm import relay\nfrom tvm import relay\nfrom tvm.runtime.vm import VirtualMachine\nfrom tvm.contrib.download import download_testdata\n\nimport numpy as np\nimport cv2\n\n# PyTorch \u5BFC\u5165\nimport torch\nimport torchvision\n"})}),"\n",(0,t.jsx)(o.h2,{id:"\u4ECE-torchvision-\u52A0\u8F7D\u9884\u8BAD\u7EC3\u7684-maskrcnn-\u5E76\u8FDB\u884C\u8DDF\u8E2A",children:"\u4ECE TorchVision \u52A0\u8F7D\u9884\u8BAD\u7EC3\u7684 MaskRCNN \u5E76\u8FDB\u884C\u8DDF\u8E2A"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-python",children:'in_size = 300\ninput_shape = (1, 3, in_size, in_size)\n\ndef do_trace(model, inp):\n    model_trace = torch.jit.trace(model, inp)\n    model_trace.eval()\n    return model_trace\n\ndef dict_to_tuple(out_dict):\n    if "masks" in out_dict.keys():\n        return out_dict["boxes"], out_dict["scores"], out_dict["labels"], out_dict["masks"]\n    return out_dict["boxes"], out_dict["scores"], out_dict["labels"]\n\nclass TraceWrapper(torch.nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n\n    def forward(self, inp):\n        out = self.model(inp)\n        return dict_to_tuple(out[0])\n\nmodel_func = torchvision.models.detection.maskrcnn_resnet50_fpn\nmodel = TraceWrapper(model_func(pretrained=True))\n\nmodel.eval()\ninp = torch.Tensor(np.random.uniform(0.0, 250.0, size=(1, 3, in_size, in_size)))\n\nwith torch.no_grad():\n    out = model(inp)\n    script_module = do_trace(model, inp)\n'})}),"\n",(0,t.jsx)(o.p,{children:"\u8F93\u51FA\u7ED3\u679C\uFF1A"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-bash",children:"Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /workspace/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n\n  0%|          | 0.00/170M [00:00<?, ?B/s]\n  9%|9         | 15.3M/170M [00:00<00:01, 160MB/s]\n 19%|#8        | 32.1M/170M [00:00<00:00, 170MB/s]\n 29%|##9       | 49.7M/170M [00:00<00:00, 176MB/s]\n 40%|####      | 68.8M/170M [00:00<00:00, 185MB/s]\n 51%|#####     | 86.4M/170M [00:00<00:00, 175MB/s]\n 61%|######1   | 104M/170M [00:00<00:00, 178MB/s]\n 71%|#######1  | 121M/170M [00:00<00:00, 169MB/s]\n 86%|########6 | 147M/170M [00:00<00:00, 199MB/s]\n100%|##########| 170M/170M [00:00<00:00, 193MB/s]\n/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3878: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  for i in range(dim)\n/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/anchor_utils.py:127: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n  for g in grid_sizes\n/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/anchor_utils.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  for g in grid_sizes\n/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/rpn.py:73: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n  A = Ax4 // 4\n/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/rpn.py:74: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n  C = AxC // A\n/usr/local/lib/python3.7/dist-packages/torchvision/ops/boxes.py:156: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  boxes_x = torch.min(boxes_x, torch.tensor(width, dtype=boxes.dtype, device=boxes.device))\n/usr/local/lib/python3.7/dist-packages/torchvision/ops/boxes.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  boxes_y = torch.min(boxes_y, torch.tensor(height, dtype=boxes.dtype, device=boxes.device))\n/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/transform.py:293: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  for s, s_orig in zip(new_size, original_size)\n/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/roi_heads.py:387: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return torch.tensor(M + 2 * padding).to(torch.float32) / torch.tensor(M).to(torch.float32)\n"})}),"\n",(0,t.jsx)(o.h2,{id:"\u4E0B\u8F7D\u6D4B\u8BD5\u56FE\u50CF\u5E76\u8FDB\u884C\u9884\u5904\u7406",children:"\u4E0B\u8F7D\u6D4B\u8BD5\u56FE\u50CF\u5E76\u8FDB\u884C\u9884\u5904\u7406"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-python",children:'img_url = (\n    "/img/docs/dmlc/web-data/master/gluoncv/detection/street_small.jpg"\n)\nimg_path = download_testdata(img_url, "test_street_small.jpg", module="data")\n\nimg = cv2.imread(img_path).astype("float32")\nimg = cv2.resize(img, (in_size, in_size))\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nimg = np.transpose(img / 255.0, [2, 0, 1])\nimg = np.expand_dims(img, axis=0)\n'})}),"\n",(0,t.jsx)(o.h2,{id:"\u5C06\u8BA1\u7B97\u56FE\u5BFC\u5165-relay",children:"\u5C06\u8BA1\u7B97\u56FE\u5BFC\u5165 Relay"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-python",children:'input_name = "input0"\nshape_list = [(input_name, input_shape)]\nmod, params = relay.frontend.from_pytorch(script_module, shape_list)\n'})}),"\n",(0,t.jsx)(o.p,{children:"\u8F93\u51FA\u7ED3\u679C\uFF1A"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-bash",children:"/workspace/python/tvm/relay/build_module.py:411: DeprecationWarning: Please use input parameter mod (tvm.IRModule) instead of deprecated parameter mod (tvm.relay.function.Function)\n  DeprecationWarning,\n"})}),"\n",(0,t.jsx)(o.h2,{id:"\u4F7F\u7528-relay-vm-\u7F16\u8BD1",children:"\u4F7F\u7528 Relay VM \u7F16\u8BD1"}),"\n",(0,t.jsx)(o.p,{children:"\u6CE8\u610F\uFF1A\u76EE\u524D\u4EC5\u652F\u6301 CPU target\u3002\u5BF9\u4E8E x86 target\uFF0C\u56E0\u4E3A TorchVision RCNN \u6A21\u578B\u4E2D\u5B58\u5728\u5927\u578B\u5BC6\u96C6\u7B97\u5B50\uFF0C\u4E3A\u53D6\u5F97\u6700\u4F73\u6027\u80FD\uFF0C\u5F3A\u70C8\u63A8\u8350\u4F7F\u7528 Intel MKL \u548C Intel OpenMP \u6765\u6784\u5EFA TVM\u3002"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-python",children:'# \u5728 x86 target\u4E0A\u6DFB\u52A0\u201C-libs=mkl\u201D\u4EE5\u83B7\u5F97\u6700\u4F73\u6027\u80FD\u3002\n# \u5BF9\u4E8E\u652F\u6301 AVX512 \u7684 x86 \u673A\u5668\uFF0C\u5B8C\u6574 target \u662F\n# "llvm -mcpu=skylake-avx512 -libs=mkl"\ntarget = "llvm"\n\nwith tvm.transform.PassContext(opt_level=3, disabled_pass=["FoldScaleAxis"]):\n    vm_exec = relay.vm.compile(mod, target=target, params=params)\n'})}),"\n",(0,t.jsx)(o.p,{children:"\u8F93\u51FA\u7ED3\u679C\uFF1A"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-bash",children:'/workspace/python/tvm/driver/build_module.py:268: UserWarning: target_host parameter is going to be deprecated. Please pass in tvm.target.Target(target, host=target_host) instead.\n  "target_host parameter is going to be deprecated. "\n'})}),"\n",(0,t.jsx)(o.h2,{id:"\u4F7F\u7528-relay-vm-\u8FDB\u884C\u63A8\u7406",children:"\u4F7F\u7528 Relay VM \u8FDB\u884C\u63A8\u7406"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-python",children:'dev = tvm.cpu()\nvm = VirtualMachine(vm_exec, dev)\nvm.set_input("main", **{input_name: img})\ntvm_res = vm.run()\n'})}),"\n",(0,t.jsx)(o.h2,{id:"\u83B7\u53D6-score-\u5927\u4E8E-09-\u7684-box",children:"\u83B7\u53D6 score \u5927\u4E8E 0.9 \u7684 box"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-python",children:'score_threshold = 0.9\nboxes = tvm_res[0].numpy().tolist()\nvalid_boxes = []\nfor i, score in enumerate(tvm_res[1].numpy().tolist()):\n    if score > score_threshold:\n        valid_boxes.append(boxes[i])\n    else:\n        break\n\nprint("Get {} valid boxes".format(len(valid_boxes)))\n'})}),"\n",(0,t.jsx)(o.p,{children:"\u8F93\u51FA\u7ED3\u679C\uFF1A"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-bash",children:"Get 9 valid boxes\n"})}),"\n",(0,t.jsxs)(o.p,{children:[(0,t.jsx)(o.strong,{children:"\u811A\u672C\u603B\u8FD0\u884C\u65F6\u957F\uFF1A"}),"\uFF082 \u5206 57.278 \u79D2\uFF09"]}),"\n",(0,t.jsx)(o.p,{children:(0,t.jsx)(o.a,{href:"https://tvm.apache.org/docs/_downloads/7795da4b258c8feff986668b95ef57ad/deploy_object_detection_pytorch.py",children:"\u4E0B\u8F7D Python \u6E90\u4EE3\u7801\uFF1Adeploy_object_detection_pytorch.py"})}),"\n",(0,t.jsx)(o.p,{children:(0,t.jsx)(o.a,{href:"https://tvm.apache.org/docs/_downloads/399e1d7889ca66b69d51655784827503/deploy_object_detection_pytorch.ipynb",children:"\u4E0B\u8F7D Jupyter Notebook\uFF1Adeploy_object_detection_pytorch.ipynb"})})]})}function p(e={}){let{wrapper:o}={...(0,s.a)(),...e.components};return o?(0,t.jsx)(o,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},21494:function(e,o,n){n.d(o,{Z:function(){return c},a:function(){return i}});var r=n(39546);let t={},s=r.createContext(t);function i(e){let o=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(o):{...o,...e}},[o,e])}function c(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),r.createElement(s.Provider,{value:o},e.children)}}}]);